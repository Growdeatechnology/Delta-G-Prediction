{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b7c9ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 16:22:26.907662: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-30 16:22:27.309030: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# from proteinbert import load_pretrained_model\n",
    "# from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a1ba5d-33dd-4bd1-91e7-bef10df3d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bff47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mutated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "633b7308-4c10-4b0a-bcf0-021517196455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(2063).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec96f1da-2aeb-4171-9f7a-09d561d96a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UniProt ID</th>\n",
       "      <th>Mutants</th>\n",
       "      <th>Protein Sequence</th>\n",
       "      <th>delta_G</th>\n",
       "      <th>mutated_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>2053</td>\n",
       "      <td>Q5NV88</td>\n",
       "      <td>H8P</td>\n",
       "      <td>NFMLTQPHSVSESPGKTVTISCTRSSGSIASNYVQWYQQRPGSSPT...</td>\n",
       "      <td>4.50</td>\n",
       "      <td>NFMLTQPPSVSESPGKTVTISCTRSSGSIASNYVQWYQQRPGSSPT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>2054</td>\n",
       "      <td>Q5NV88</td>\n",
       "      <td>H8S</td>\n",
       "      <td>NFMLTQPHSVSESPGKTVTISCTRSSGSIASNYVQWYQQRPGSSPT...</td>\n",
       "      <td>4.50</td>\n",
       "      <td>NFMLTQPSSVSESPGKTVTISCTRSSGSIASNYVQWYQQRPGSSPT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>2055</td>\n",
       "      <td>Q70YQ6</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MIVSMIAALANNRVIGLDNKMPWHLPAELQLFKRATLGKPIVMGRN...</td>\n",
       "      <td>-3.45</td>\n",
       "      <td>MIVSMIAALANNRVIGLDNKMPWHLPAELQLFKRATLGKPIVMGRN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047</th>\n",
       "      <td>2056</td>\n",
       "      <td>Q75QQ5</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MNSYQNKNEYEILNAPSNNTNMPNRYPFANDPNAMMKNGNYKDWLD...</td>\n",
       "      <td>23.10</td>\n",
       "      <td>MNSYQNKNEYEILNAPSNNTNMPNRYPFANDPNAMMKNGNYKDWLD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>2057</td>\n",
       "      <td>Q75QQ5</td>\n",
       "      <td>I189L</td>\n",
       "      <td>MNSYQNKNEYEILNAPSNNTNMPNRYPFANDPNAMMKNGNYKDWLD...</td>\n",
       "      <td>22.78</td>\n",
       "      <td>MNSYQNKNEYEILNAPSNNTNMPNRYPFANDPNAMMKNGNYKDWLD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2049</th>\n",
       "      <td>2058</td>\n",
       "      <td>Q75QQ5</td>\n",
       "      <td>L175V</td>\n",
       "      <td>MNSYQNKNEYEILNAPSNNTNMPNRYPFANDPNAMMKNGNYKDWLD...</td>\n",
       "      <td>12.61</td>\n",
       "      <td>MNSYQNKNEYEILNAPSNNTNMPNRYPFANDPNAMMKNGNYKDWLD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2050</th>\n",
       "      <td>2059</td>\n",
       "      <td>Q7LZZ4</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...</td>\n",
       "      <td>12.70</td>\n",
       "      <td>EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>2060</td>\n",
       "      <td>Q7LZZ4</td>\n",
       "      <td>C350S</td>\n",
       "      <td>EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...</td>\n",
       "      <td>9.30</td>\n",
       "      <td>EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2052</th>\n",
       "      <td>2061</td>\n",
       "      <td>Q7LZZ4</td>\n",
       "      <td>G288A</td>\n",
       "      <td>EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...</td>\n",
       "      <td>9.30</td>\n",
       "      <td>EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>2062</td>\n",
       "      <td>Q7LZZ4</td>\n",
       "      <td>E291D</td>\n",
       "      <td>EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...</td>\n",
       "      <td>7.20</td>\n",
       "      <td>EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>2063</td>\n",
       "      <td>Q7LZZ4</td>\n",
       "      <td>R299A</td>\n",
       "      <td>EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>2064</td>\n",
       "      <td>Q7M089</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MANLSYWLLALFVAMWTDVGLCKKRPKPGGWNTGGSRYPGQGSPGG...</td>\n",
       "      <td>6.10</td>\n",
       "      <td>MANLSYWLLALFVAMWTDVGLCKKRPKPGGWNTGGSRYPGQGSPGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2056</th>\n",
       "      <td>2065</td>\n",
       "      <td>Q7SID9</td>\n",
       "      <td>H69A</td>\n",
       "      <td>STGPVAPLPTPPNFPNDIALFQQAYQNWSKEIMLDATWVCSPKTPQ...</td>\n",
       "      <td>1.20</td>\n",
       "      <td>STGPVAPLPTPPNFPNDIALFQQAYQNWSKEIMLDATWVCSPKTPQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>2066</td>\n",
       "      <td>Q7SIF8</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>QKPKLLYCSNGGYFLRIFPDGKVDGTRDRSDPYIQLQFYAESVGEV...</td>\n",
       "      <td>6.10</td>\n",
       "      <td>QKPKLLYCSNGGYFLRIFPDGKVDGTRDRSDPYIQLQFYAESVGEV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>2067</td>\n",
       "      <td>Q7SIG1</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MPLDPVIQQVLDQLNRMPAPDYKHLSAQQFRSQQSLFPPVKKEPVA...</td>\n",
       "      <td>4.30</td>\n",
       "      <td>MPLDPVIQQVLDQLNRMPAPDYKHLSAQQFRSQQSLFPPVKKEPVA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>2068</td>\n",
       "      <td>Q7Z7M3</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MPTSSSTKKTQLQLEHLLLDLQMILNGINNYKNPKLTRMLTFKFYM...</td>\n",
       "      <td>9.30</td>\n",
       "      <td>MPTSSSTKKTQLQLEHLLLDLQMILNGINNYKNPKLTRMLTFKFYM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>2069</td>\n",
       "      <td>Q7Z7M3</td>\n",
       "      <td>L38M L39S</td>\n",
       "      <td>MPTSSSTKKTQLQLEHLLLDLQMILNGINNYKNPKLTRMLTFKFYM...</td>\n",
       "      <td>9.30</td>\n",
       "      <td>MPTSSSTKKTQLQLEHLLLDLQMILNGINNYKNPKLTMSLTFKFYM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>2070</td>\n",
       "      <td>Q8ENY1</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MKSQTFTITAETGVHARPATLLVNKAGQFDSEIEVSYKGKQVNLKS...</td>\n",
       "      <td>1.40</td>\n",
       "      <td>MKSQTFTITAETGVHARPATLLVNKAGQFDSEIEVSYKGKQVNLKS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>2071</td>\n",
       "      <td>Q8VP89</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MASKDFHIVAETGIHARPATLLVQTASKFASDITLEYKGKAVNLKS...</td>\n",
       "      <td>5.40</td>\n",
       "      <td>MASKDFHIVAETGIHARPATLLVQTASKFASDITLEYKGKAVNLKS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>2077</td>\n",
       "      <td>Q8Z9H3</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MADRNLRDLLAPWVAGLPARELREMTLDSRVAAAGDLFVAVVGHQA...</td>\n",
       "      <td>3.27</td>\n",
       "      <td>MADRNLRDLLAPWVAGLPARELREMTLDSRVAAAGDLFVAVVGHQA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>2078</td>\n",
       "      <td>Q95211</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MAHLGYWMLLLFVATWSDVGLCKKRPKPGGGWNTGGSRYPGQSSPG...</td>\n",
       "      <td>7.70</td>\n",
       "      <td>MAHLGYWMLLLFVATWSDVGLCKKRPKPGGGWNTGGSRYPGQSSPG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>2079</td>\n",
       "      <td>Q97ZL0</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MKKWSDTEVFEMLKRMYARVYGLVQGVGFRKFVQIHAIRLGIKGYA...</td>\n",
       "      <td>11.64</td>\n",
       "      <td>MKKWSDTEVFEMLKRMYARVYGLVQGVGFRKFVQIHAIRLGIKGYA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>2080</td>\n",
       "      <td>Q9EYL5</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MTKKLLTLFIVSMLILTACGKKESATTSSKNGKPLVVVYGDYKCPY...</td>\n",
       "      <td>8.00</td>\n",
       "      <td>MTKKLLTLFIVSMLILTACGKKESATTSSKNGKPLVVVYGDYKCPY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>2081</td>\n",
       "      <td>Q9H334</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...</td>\n",
       "      <td>8.90</td>\n",
       "      <td>MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>2082</td>\n",
       "      <td>Q9H334</td>\n",
       "      <td>H520A</td>\n",
       "      <td>MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...</td>\n",
       "      <td>20.20</td>\n",
       "      <td>MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>2083</td>\n",
       "      <td>Q9H334</td>\n",
       "      <td>S518C</td>\n",
       "      <td>MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...</td>\n",
       "      <td>1.60</td>\n",
       "      <td>MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>2084</td>\n",
       "      <td>Q9H334</td>\n",
       "      <td>V539C</td>\n",
       "      <td>MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...</td>\n",
       "      <td>0.60</td>\n",
       "      <td>MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>2085</td>\n",
       "      <td>Q9K8D2</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MAEKTFTITAETGIHARPATQLVNKAGQYSSEITLEYKGKAVNLKS...</td>\n",
       "      <td>5.30</td>\n",
       "      <td>MAEKTFTITAETGIHARPATQLVNKAGQYSSEITLEYKGKAVNLKS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>2086</td>\n",
       "      <td>Q9NAV7</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MGFKQDIATLRGDLRTYAQDIFLAFLNKYPDEKRNFKNYVGKSDQE...</td>\n",
       "      <td>2.21</td>\n",
       "      <td>MGFKQDIATLRGDLRTYAQDIFLAFLNKYPDEKRNFKNYVGKSDQE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>2087</td>\n",
       "      <td>Q9NAV8</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MGFKQDIATIRGDLRTYAQDIFLAFLNKYPDERRYFKNYVGKSDQE...</td>\n",
       "      <td>6.80</td>\n",
       "      <td>MGFKQDIATIRGDLRTYAQDIFLAFLNKYPDERRYFKNYVGKSDQE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>2088</td>\n",
       "      <td>Q9NAV8</td>\n",
       "      <td>H56V</td>\n",
       "      <td>MGFKQDIATIRGDLRTYAQDIFLAFLNKYPDERRYFKNYVGKSDQE...</td>\n",
       "      <td>7.52</td>\n",
       "      <td>MGFKQDIATIRGDLRTYAQDIFLAFLNKYPDERRYFKNYVGKSDQE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>2089</td>\n",
       "      <td>Q9NYA1</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MDPAGGPRGVLPRPCRVLVLLNPRGGKGKALQLFRSHVQPLLAEAE...</td>\n",
       "      <td>1.75</td>\n",
       "      <td>MDPAGGPRGVLPRPCRVLVLLNPRGGKGKALQLFRSHVQPLLAEAE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>2090</td>\n",
       "      <td>Q9R2S1</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MEIEQLIVEAKKARELAYVPYSKFPVGAALLTKGGSVYRGCNIENA...</td>\n",
       "      <td>4.30</td>\n",
       "      <td>MEIEQLIVEAKKARELAYVPYSKFPVGAALLTKGGSVYRGCNIENA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>2091</td>\n",
       "      <td>Q9R782</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MKLWFSTLKLKKAA</td>\n",
       "      <td>5.20</td>\n",
       "      <td>MKLWFSTLKLKKAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>2092</td>\n",
       "      <td>Q9R782</td>\n",
       "      <td>S140C</td>\n",
       "      <td>MKLWFSTLKLKKAA</td>\n",
       "      <td>4.30</td>\n",
       "      <td>MKLWFSTLKLKKAAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2079</th>\n",
       "      <td>2093</td>\n",
       "      <td>Q9R782</td>\n",
       "      <td>S277C</td>\n",
       "      <td>MKLWFSTLKLKKAA</td>\n",
       "      <td>4.10</td>\n",
       "      <td>MKLWFSTLKLKKAAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>2094</td>\n",
       "      <td>Q9R782</td>\n",
       "      <td>S140C S277C</td>\n",
       "      <td>MKLWFSTLKLKKAA</td>\n",
       "      <td>5.10</td>\n",
       "      <td>MKLWFSTLKLKKAACC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>2095</td>\n",
       "      <td>Q9S3M0</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MDVEKLIAESKKAREQAYVPYSKFPVGAALLAEDGTIYHGCNIENS...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>MDVEKLIAESKKAREQAYVPYSKFPVGAALLAEDGTIYHGCNIENS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>2096</td>\n",
       "      <td>Q9S8W1</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>RNILMNGEGLYAGQSLDVEPYHFIMQEDCN</td>\n",
       "      <td>12.90</td>\n",
       "      <td>RNILMNGEGLYAGQSLDVEPYHFIMQEDCN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>2097</td>\n",
       "      <td>Q9WMV8</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>HQALSPRTLNAWVKVIEEKAFSPEVIPMFSALSEGATPQDLNTMLN...</td>\n",
       "      <td>4.71</td>\n",
       "      <td>HQALSPRTLNAWVKVIEEKAFSPEVIPMFSALSEGATPQDLNTMLN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>2098</td>\n",
       "      <td>Q9WZ62</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MKKLLAVSILLVSIVIFSGAIDEIKSRGYLLVGLSADFPPFEFVDE...</td>\n",
       "      <td>12.89</td>\n",
       "      <td>MKKLLAVSILLVSIVIFSGAIDEIKSRGYLLVGLSADFPPFEFVDE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>2099</td>\n",
       "      <td>R9S082</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "      <td>5.60</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2086</th>\n",
       "      <td>2100</td>\n",
       "      <td>R9S082</td>\n",
       "      <td>W8F</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "      <td>4.70</td>\n",
       "      <td>MGLSDGEFQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>2101</td>\n",
       "      <td>R9S082</td>\n",
       "      <td>W15F</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "      <td>4.50</td>\n",
       "      <td>MGLSDGEWQLVLNVFGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>2102</td>\n",
       "      <td>R9S082</td>\n",
       "      <td>M132A</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "      <td>3.40</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>2103</td>\n",
       "      <td>R9S082</td>\n",
       "      <td>F124K</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "      <td>3.50</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>2104</td>\n",
       "      <td>R9S082</td>\n",
       "      <td>A131K</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "      <td>3.50</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>2105</td>\n",
       "      <td>R9S082</td>\n",
       "      <td>A131L</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "      <td>4.70</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>2106</td>\n",
       "      <td>R9S082</td>\n",
       "      <td>H37Q</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "      <td>4.80</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGQPETLEKFDK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>2107</td>\n",
       "      <td>R9S082</td>\n",
       "      <td>V69T</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 UniProt ID      Mutants  \\\n",
       "2044        2053     Q5NV88          H8P   \n",
       "2045        2054     Q5NV88          H8S   \n",
       "2046        2055     Q70YQ6    wild-type   \n",
       "2047        2056     Q75QQ5    wild-type   \n",
       "2048        2057     Q75QQ5        I189L   \n",
       "2049        2058     Q75QQ5        L175V   \n",
       "2050        2059     Q7LZZ4    wild-type   \n",
       "2051        2060     Q7LZZ4        C350S   \n",
       "2052        2061     Q7LZZ4        G288A   \n",
       "2053        2062     Q7LZZ4        E291D   \n",
       "2054        2063     Q7LZZ4        R299A   \n",
       "2055        2064     Q7M089    wild-type   \n",
       "2056        2065     Q7SID9         H69A   \n",
       "2057        2066     Q7SIF8    wild-type   \n",
       "2058        2067     Q7SIG1    wild-type   \n",
       "2059        2068     Q7Z7M3    wild-type   \n",
       "2060        2069     Q7Z7M3    L38M L39S   \n",
       "2061        2070     Q8ENY1    wild-type   \n",
       "2062        2071     Q8VP89    wild-type   \n",
       "2063        2077     Q8Z9H3    wild-type   \n",
       "2064        2078     Q95211    wild-type   \n",
       "2065        2079     Q97ZL0    wild-type   \n",
       "2066        2080     Q9EYL5    wild-type   \n",
       "2067        2081     Q9H334    wild-type   \n",
       "2068        2082     Q9H334        H520A   \n",
       "2069        2083     Q9H334        S518C   \n",
       "2070        2084     Q9H334        V539C   \n",
       "2071        2085     Q9K8D2    wild-type   \n",
       "2072        2086     Q9NAV7    wild-type   \n",
       "2073        2087     Q9NAV8    wild-type   \n",
       "2074        2088     Q9NAV8         H56V   \n",
       "2075        2089     Q9NYA1    wild-type   \n",
       "2076        2090     Q9R2S1    wild-type   \n",
       "2077        2091     Q9R782    wild-type   \n",
       "2078        2092     Q9R782        S140C   \n",
       "2079        2093     Q9R782        S277C   \n",
       "2080        2094     Q9R782  S140C S277C   \n",
       "2081        2095     Q9S3M0    wild-type   \n",
       "2082        2096     Q9S8W1    wild-type   \n",
       "2083        2097     Q9WMV8    wild-type   \n",
       "2084        2098     Q9WZ62    wild-type   \n",
       "2085        2099     R9S082    wild-type   \n",
       "2086        2100     R9S082          W8F   \n",
       "2087        2101     R9S082         W15F   \n",
       "2088        2102     R9S082        M132A   \n",
       "2089        2103     R9S082        F124K   \n",
       "2090        2104     R9S082        A131K   \n",
       "2091        2105     R9S082        A131L   \n",
       "2092        2106     R9S082         H37Q   \n",
       "2093        2107     R9S082         V69T   \n",
       "\n",
       "                                       Protein Sequence  delta_G  \\\n",
       "2044  NFMLTQPHSVSESPGKTVTISCTRSSGSIASNYVQWYQQRPGSSPT...     4.50   \n",
       "2045  NFMLTQPHSVSESPGKTVTISCTRSSGSIASNYVQWYQQRPGSSPT...     4.50   \n",
       "2046  MIVSMIAALANNRVIGLDNKMPWHLPAELQLFKRATLGKPIVMGRN...    -3.45   \n",
       "2047  MNSYQNKNEYEILNAPSNNTNMPNRYPFANDPNAMMKNGNYKDWLD...    23.10   \n",
       "2048  MNSYQNKNEYEILNAPSNNTNMPNRYPFANDPNAMMKNGNYKDWLD...    22.78   \n",
       "2049  MNSYQNKNEYEILNAPSNNTNMPNRYPFANDPNAMMKNGNYKDWLD...    12.61   \n",
       "2050  EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...    12.70   \n",
       "2051  EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...     9.30   \n",
       "2052  EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...     9.30   \n",
       "2053  EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...     7.20   \n",
       "2054  EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...     9.00   \n",
       "2055  MANLSYWLLALFVAMWTDVGLCKKRPKPGGWNTGGSRYPGQGSPGG...     6.10   \n",
       "2056  STGPVAPLPTPPNFPNDIALFQQAYQNWSKEIMLDATWVCSPKTPQ...     1.20   \n",
       "2057  QKPKLLYCSNGGYFLRIFPDGKVDGTRDRSDPYIQLQFYAESVGEV...     6.10   \n",
       "2058  MPLDPVIQQVLDQLNRMPAPDYKHLSAQQFRSQQSLFPPVKKEPVA...     4.30   \n",
       "2059  MPTSSSTKKTQLQLEHLLLDLQMILNGINNYKNPKLTRMLTFKFYM...     9.30   \n",
       "2060  MPTSSSTKKTQLQLEHLLLDLQMILNGINNYKNPKLTRMLTFKFYM...     9.30   \n",
       "2061  MKSQTFTITAETGVHARPATLLVNKAGQFDSEIEVSYKGKQVNLKS...     1.40   \n",
       "2062  MASKDFHIVAETGIHARPATLLVQTASKFASDITLEYKGKAVNLKS...     5.40   \n",
       "2063  MADRNLRDLLAPWVAGLPARELREMTLDSRVAAAGDLFVAVVGHQA...     3.27   \n",
       "2064  MAHLGYWMLLLFVATWSDVGLCKKRPKPGGGWNTGGSRYPGQSSPG...     7.70   \n",
       "2065  MKKWSDTEVFEMLKRMYARVYGLVQGVGFRKFVQIHAIRLGIKGYA...    11.64   \n",
       "2066  MTKKLLTLFIVSMLILTACGKKESATTSSKNGKPLVVVYGDYKCPY...     8.00   \n",
       "2067  MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...     8.90   \n",
       "2068  MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...    20.20   \n",
       "2069  MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...     1.60   \n",
       "2070  MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...     0.60   \n",
       "2071  MAEKTFTITAETGIHARPATQLVNKAGQYSSEITLEYKGKAVNLKS...     5.30   \n",
       "2072  MGFKQDIATLRGDLRTYAQDIFLAFLNKYPDEKRNFKNYVGKSDQE...     2.21   \n",
       "2073  MGFKQDIATIRGDLRTYAQDIFLAFLNKYPDERRYFKNYVGKSDQE...     6.80   \n",
       "2074  MGFKQDIATIRGDLRTYAQDIFLAFLNKYPDERRYFKNYVGKSDQE...     7.52   \n",
       "2075  MDPAGGPRGVLPRPCRVLVLLNPRGGKGKALQLFRSHVQPLLAEAE...     1.75   \n",
       "2076  MEIEQLIVEAKKARELAYVPYSKFPVGAALLTKGGSVYRGCNIENA...     4.30   \n",
       "2077                                     MKLWFSTLKLKKAA     5.20   \n",
       "2078                                     MKLWFSTLKLKKAA     4.30   \n",
       "2079                                     MKLWFSTLKLKKAA     4.10   \n",
       "2080                                     MKLWFSTLKLKKAA     5.10   \n",
       "2081  MDVEKLIAESKKAREQAYVPYSKFPVGAALLAEDGTIYHGCNIENS...     3.00   \n",
       "2082                     RNILMNGEGLYAGQSLDVEPYHFIMQEDCN    12.90   \n",
       "2083  HQALSPRTLNAWVKVIEEKAFSPEVIPMFSALSEGATPQDLNTMLN...     4.71   \n",
       "2084  MKKLLAVSILLVSIVIFSGAIDEIKSRGYLLVGLSADFPPFEFVDE...    12.89   \n",
       "2085  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...     5.60   \n",
       "2086  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...     4.70   \n",
       "2087  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...     4.50   \n",
       "2088  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...     3.40   \n",
       "2089  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...     3.50   \n",
       "2090  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...     3.50   \n",
       "2091  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...     4.70   \n",
       "2092  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...     4.80   \n",
       "2093  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...     5.00   \n",
       "\n",
       "                                       mutated_sequence  \n",
       "2044  NFMLTQPPSVSESPGKTVTISCTRSSGSIASNYVQWYQQRPGSSPT...  \n",
       "2045  NFMLTQPSSVSESPGKTVTISCTRSSGSIASNYVQWYQQRPGSSPT...  \n",
       "2046  MIVSMIAALANNRVIGLDNKMPWHLPAELQLFKRATLGKPIVMGRN...  \n",
       "2047  MNSYQNKNEYEILNAPSNNTNMPNRYPFANDPNAMMKNGNYKDWLD...  \n",
       "2048  MNSYQNKNEYEILNAPSNNTNMPNRYPFANDPNAMMKNGNYKDWLD...  \n",
       "2049  MNSYQNKNEYEILNAPSNNTNMPNRYPFANDPNAMMKNGNYKDWLD...  \n",
       "2050  EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...  \n",
       "2051  EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...  \n",
       "2052  EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...  \n",
       "2053  EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...  \n",
       "2054  EEPLPDRAVPIYVAGFLALPRGPFFVGLIAVQLLRDRSLVAERRRQ...  \n",
       "2055  MANLSYWLLALFVAMWTDVGLCKKRPKPGGWNTGGSRYPGQGSPGG...  \n",
       "2056  STGPVAPLPTPPNFPNDIALFQQAYQNWSKEIMLDATWVCSPKTPQ...  \n",
       "2057  QKPKLLYCSNGGYFLRIFPDGKVDGTRDRSDPYIQLQFYAESVGEV...  \n",
       "2058  MPLDPVIQQVLDQLNRMPAPDYKHLSAQQFRSQQSLFPPVKKEPVA...  \n",
       "2059  MPTSSSTKKTQLQLEHLLLDLQMILNGINNYKNPKLTRMLTFKFYM...  \n",
       "2060  MPTSSSTKKTQLQLEHLLLDLQMILNGINNYKNPKLTMSLTFKFYM...  \n",
       "2061  MKSQTFTITAETGVHARPATLLVNKAGQFDSEIEVSYKGKQVNLKS...  \n",
       "2062  MASKDFHIVAETGIHARPATLLVQTASKFASDITLEYKGKAVNLKS...  \n",
       "2063  MADRNLRDLLAPWVAGLPARELREMTLDSRVAAAGDLFVAVVGHQA...  \n",
       "2064  MAHLGYWMLLLFVATWSDVGLCKKRPKPGGGWNTGGSRYPGQSSPG...  \n",
       "2065  MKKWSDTEVFEMLKRMYARVYGLVQGVGFRKFVQIHAIRLGIKGYA...  \n",
       "2066  MTKKLLTLFIVSMLILTACGKKESATTSSKNGKPLVVVYGDYKCPY...  \n",
       "2067  MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...  \n",
       "2068  MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...  \n",
       "2069  MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...  \n",
       "2070  MMQESGTETKSNGSAIQNGSGGSNHLLECGGLREGRSNGETPAVDI...  \n",
       "2071  MAEKTFTITAETGIHARPATQLVNKAGQYSSEITLEYKGKAVNLKS...  \n",
       "2072  MGFKQDIATLRGDLRTYAQDIFLAFLNKYPDEKRNFKNYVGKSDQE...  \n",
       "2073  MGFKQDIATIRGDLRTYAQDIFLAFLNKYPDERRYFKNYVGKSDQE...  \n",
       "2074  MGFKQDIATIRGDLRTYAQDIFLAFLNKYPDERRYFKNYVGKSDQE...  \n",
       "2075  MDPAGGPRGVLPRPCRVLVLLNPRGGKGKALQLFRSHVQPLLAEAE...  \n",
       "2076  MEIEQLIVEAKKARELAYVPYSKFPVGAALLTKGGSVYRGCNIENA...  \n",
       "2077                                     MKLWFSTLKLKKAA  \n",
       "2078                                    MKLWFSTLKLKKAAC  \n",
       "2079                                    MKLWFSTLKLKKAAC  \n",
       "2080                                   MKLWFSTLKLKKAACC  \n",
       "2081  MDVEKLIAESKKAREQAYVPYSKFPVGAALLAEDGTIYHGCNIENS...  \n",
       "2082                     RNILMNGEGLYAGQSLDVEPYHFIMQEDCN  \n",
       "2083  HQALSPRTLNAWVKVIEEKAFSPEVIPMFSALSEGATPQDLNTMLN...  \n",
       "2084  MKKLLAVSILLVSIVIFSGAIDEIKSRGYLLVGLSADFPPFEFVDE...  \n",
       "2085  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...  \n",
       "2086  MGLSDGEFQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...  \n",
       "2087  MGLSDGEWQLVLNVFGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...  \n",
       "2088  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...  \n",
       "2089  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...  \n",
       "2090  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...  \n",
       "2091  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...  \n",
       "2092  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGQPETLEKFDK...  \n",
       "2093  MGLSDGEWQLVLNVWGKVETDLAGHGQEVLIRLFKGHPETLEKFDK...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0686d930-2783-4fe6-8fd3-95dd8807dc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                       2077\n",
       "UniProt ID                                                     Q8Z9H3\n",
       "Mutants                                                     wild-type\n",
       "Protein Sequence    MADRNLRDLLAPWVAGLPARELREMTLDSRVAAAGDLFVAVVGHQA...\n",
       "delta_G                                                          3.27\n",
       "mutated_sequence    MADRNLRDLLAPWVAGLPARELREMTLDSRVAAAGDLFVAVVGHQA...\n",
       "Name: 2063, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[2063]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645299bc-1cc8-45b7-8e98-96b39d151e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_sequences = df['mutated_sequence']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f1200-b44d-4304-8af1-2e9641f1fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = -1\n",
    "max_sequence_index = -1\n",
    "\n",
    "for index, seq in enumerate(protein_sequences):\n",
    "    if len(seq) > max_sequence_length:\n",
    "        max_sequence_length = len(seq)\n",
    "        max_sequence_index = index\n",
    "\n",
    "# Print the results\n",
    "print(\"Maximum Sequence Length:\", max_sequence_length)\n",
    "print(\"Index of Sequence with Maximum Length:\", max_sequence_index)\n",
    "print(\"Sequence with Maximum Length:\", protein_sequences[max_sequence_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb7c25-f599-495e-bc22-d07dce12715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max(len(seq) for seq in protein_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e41cc-b263-450e-aa8b-fc0905fd2197",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7046d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame\n",
    "protein_sequences = df['mutated_sequence']\n",
    "delta_g_values = df['delta_G']\n",
    "\n",
    "# Define the amino acids and their order\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def one_hot_encode_sequence(sequence):\n",
    "    # Create a dictionary to map each amino acid to an index\n",
    "    aa_to_index = {aa: i for i, aa in enumerate(amino_acids)}\n",
    "    \n",
    "    # Initialize an array of zeros with the shape (sequence_length, number_of_amino_acids)\n",
    "    sequence_length = len(sequence)\n",
    "    one_hot_encoded = np.zeros((sequence_length, len(amino_acids)), dtype=int)\n",
    "    \n",
    "    # Set the appropriate index to 1 for each amino acid in the sequence\n",
    "    for i, aa in enumerate(sequence):\n",
    "        one_hot_encoded[i, aa_to_index[aa]] = 1\n",
    "    \n",
    "    return one_hot_encoded\n",
    "\n",
    "# One-hot encode all the protein sequences in the DataFrame\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming you have a list of one-hot encoded sequences\n",
    "encoded_sequences = [one_hot_encode_sequence(seq) for seq in protein_sequences]\n",
    "\n",
    "#Find the maximum sequence length\n",
    "max_sequence_length = max(len(seq) for seq in encoded_sequences)\n",
    "\n",
    "# Pad sequences to the maximum length\n",
    "padded_sequences = pad_sequences(encoded_sequences, maxlen=max_sequence_length, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "# Convert the padded_sequences list to a numpy array\n",
    "X = np.array(padded_sequences)\n",
    "\n",
    "# Convert the delta_g_values to a numpy array\n",
    "y = np.array(delta_g_values)\n",
    "del protein_sequences\n",
    "del delta_g_values\n",
    "del encoded_sequences\n",
    "del padded_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63086ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 70% training and 30% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3,random_state=42)\n",
    "\n",
    "# # Split the remaining data (30%) into 10% validation and 20% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.67, random_state=42)\n",
    "\n",
    "# # Print the shapes of the datasets\n",
    "print(\"Training data shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation data shapes:\", X_val.shape, y_val.shape)\n",
    "print(\"Test data shapes:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09b924f3-4fc3-4b10-9b83-7c2590354114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97ed8e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(744, 7073, 20)\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 22.9050 - val_loss: 12.1013\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 12.1844 - val_loss: 10.5143\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 11.4881 - val_loss: 9.3475\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 10.2871 - val_loss: 8.5789\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 9.1405 - val_loss: 7.5712\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 8.1884 - val_loss: 6.5328\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 6.4102 - val_loss: 6.0243\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 5.3728 - val_loss: 5.5561\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 4.4237 - val_loss: 4.9081\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 4.3240 - val_loss: 5.0346\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 3.5269 - val_loss: 5.0546\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 3.0905 - val_loss: 5.0979\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 1s 43ms/step - loss: 3.0120 - val_loss: 4.6784\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 3.0437 - val_loss: 5.1042\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.5540 - val_loss: 5.2918\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.7026 - val_loss: 4.7816\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.4806 - val_loss: 5.0961\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.4451 - val_loss: 5.2562\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.1345 - val_loss: 4.6685\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.1094 - val_loss: 5.1991\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.2676 - val_loss: 4.9052\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.3849 - val_loss: 4.7525\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.1626 - val_loss: 4.9103\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.1416 - val_loss: 4.7354\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 2.3553 - val_loss: 5.4419\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.3347 - val_loss: 4.5947\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.6329 - val_loss: 4.7315\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.4219 - val_loss: 5.3334\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.5083 - val_loss: 4.5328\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.9701 - val_loss: 4.6879\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 2.6100 - val_loss: 4.6468\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.3462 - val_loss: 4.7401\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.8927 - val_loss: 5.0261\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.0552 - val_loss: 4.9431\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.9778 - val_loss: 5.2295\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 2.3051 - val_loss: 5.2552\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.3140 - val_loss: 4.7562\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 2.5167 - val_loss: 4.9123\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.5342 - val_loss: 4.8794\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.0368 - val_loss: 4.6898\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 2.0726 - val_loss: 5.4059\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.2093 - val_loss: 5.2244\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.1927 - val_loss: 4.7010\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 2.1783 - val_loss: 4.9421\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 1.9355 - val_loss: 4.9262\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 1.7191 - val_loss: 4.4663\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.8768 - val_loss: 5.4011\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.8861 - val_loss: 4.5834\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.9992 - val_loss: 4.5733\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.7973 - val_loss: 5.0167\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.7767 - val_loss: 4.9268\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.9162 - val_loss: 4.8916\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.0751 - val_loss: 4.6029\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6803 - val_loss: 4.9665\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.9222 - val_loss: 5.2033\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 1s 43ms/step - loss: 1.9154 - val_loss: 4.9014\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.7506 - val_loss: 4.6239\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6417 - val_loss: 4.7285\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.8068 - val_loss: 4.6402\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.8099 - val_loss: 4.7500\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.7920 - val_loss: 4.6295\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.7802 - val_loss: 4.7977\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.5330 - val_loss: 4.6332\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5452 - val_loss: 4.7941\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6656 - val_loss: 4.7509\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6092 - val_loss: 4.9191\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5618 - val_loss: 4.9643\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.7103 - val_loss: 4.8516\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.7083 - val_loss: 4.6784\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.9345 - val_loss: 4.6296\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.0678 - val_loss: 4.7488\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.2501 - val_loss: 5.2422\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.8985 - val_loss: 4.7737\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.2899 - val_loss: 5.6427\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.8681 - val_loss: 4.7935\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6005 - val_loss: 4.6280\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5989 - val_loss: 4.8190\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6304 - val_loss: 4.6780\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6681 - val_loss: 4.5567\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6919 - val_loss: 4.5273\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6147 - val_loss: 4.9673\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.5012 - val_loss: 4.9936\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 1.6541 - val_loss: 4.7635\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6908 - val_loss: 4.6675\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.5095 - val_loss: 4.5436\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5647 - val_loss: 4.4556\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.8728 - val_loss: 4.8357\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.7010 - val_loss: 4.6999\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.3686 - val_loss: 4.9993\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4037 - val_loss: 4.8092\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4970 - val_loss: 4.5805\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6174 - val_loss: 4.9490\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5286 - val_loss: 4.7843\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.4831 - val_loss: 4.5194\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.3899 - val_loss: 4.8093\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.4438 - val_loss: 4.9597\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.3937 - val_loss: 4.8665\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.3502 - val_loss: 4.7241\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6398 - val_loss: 5.2704\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.7335 - val_loss: 4.6308\n",
      "12/12 [==============================] - 0s 17ms/step\n",
      "0.6424759274438134\n",
      "(744, 7073, 20)\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 21.2410 - val_loss: 10.4856\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 11.8434 - val_loss: 10.2703\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 10.3924 - val_loss: 10.0472\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 8.7722 - val_loss: 9.4262\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 7.7890 - val_loss: 8.6451\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 6.6076 - val_loss: 8.8525\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 6.3548 - val_loss: 7.8165\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 5.0949 - val_loss: 7.9830\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 4.6172 - val_loss: 7.3642\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 4.0945 - val_loss: 7.5689\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 3.7221 - val_loss: 7.4983\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 3.4620 - val_loss: 8.0726\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.9969 - val_loss: 8.1039\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.8818 - val_loss: 7.8786\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.9961 - val_loss: 8.2753\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 2.6836 - val_loss: 7.6970\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.7712 - val_loss: 7.8083\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.5745 - val_loss: 8.0824\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.3753 - val_loss: 7.9460\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 2.1202 - val_loss: 8.1468\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.9313 - val_loss: 7.8297\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.1678 - val_loss: 7.9500\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.1401 - val_loss: 7.8383\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.2015 - val_loss: 8.1118\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.1048 - val_loss: 7.5343\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.9326 - val_loss: 7.6390\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.0148 - val_loss: 7.7594\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.7953 - val_loss: 7.4756\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 1.9360 - val_loss: 7.7292\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.7636 - val_loss: 7.7423\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.7401 - val_loss: 8.0030\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 1.8830 - val_loss: 8.1167\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 1.9599 - val_loss: 7.7817\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.0980 - val_loss: 7.7657\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6343 - val_loss: 7.4838\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.8053 - val_loss: 7.5803\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 1.8053 - val_loss: 8.1290\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.9680 - val_loss: 8.3109\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.9615 - val_loss: 7.6325\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.9038 - val_loss: 7.6787\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 1.6229 - val_loss: 7.3634\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.7741 - val_loss: 7.7811\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6047 - val_loss: 7.8219\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.7257 - val_loss: 8.2224\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.7407 - val_loss: 7.6978\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.8413 - val_loss: 7.6248\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6620 - val_loss: 8.0135\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.8118 - val_loss: 7.6274\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.7011 - val_loss: 7.6421\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6378 - val_loss: 7.9019\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6233 - val_loss: 7.7447\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6199 - val_loss: 7.7119\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6152 - val_loss: 7.7289\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6327 - val_loss: 7.7027\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.4422 - val_loss: 7.4246\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.4768 - val_loss: 7.8320\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 1.3476 - val_loss: 7.8813\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 1.4947 - val_loss: 7.6102\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6014 - val_loss: 8.2200\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6392 - val_loss: 8.2428\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.5898 - val_loss: 7.9225\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5503 - val_loss: 7.6986\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5941 - val_loss: 7.6093\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5446 - val_loss: 8.0878\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.7869 - val_loss: 7.4893\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.3164 - val_loss: 7.6415\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.4846 - val_loss: 7.7245\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.2593 - val_loss: 8.1019\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4754 - val_loss: 7.8898\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4738 - val_loss: 7.8050\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.2089 - val_loss: 7.7333\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4604 - val_loss: 7.7963\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5747 - val_loss: 7.7701\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6619 - val_loss: 7.6769\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4394 - val_loss: 7.5910\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.4882 - val_loss: 7.8130\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 1.5281 - val_loss: 7.8449\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.3850 - val_loss: 7.8388\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4111 - val_loss: 7.2969\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.3797 - val_loss: 7.4332\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.4119 - val_loss: 7.7895\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.3282 - val_loss: 7.9291\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.3391 - val_loss: 7.9224\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.2645 - val_loss: 7.8915\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4581 - val_loss: 7.9000\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.3825 - val_loss: 7.5723\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 1.2758 - val_loss: 7.7718\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.3305 - val_loss: 7.5346\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.2445 - val_loss: 7.8880\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4893 - val_loss: 7.8143\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.3562 - val_loss: 7.6279\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.2753 - val_loss: 7.7970\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4212 - val_loss: 7.7952\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6291 - val_loss: 7.6770\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.2734 - val_loss: 7.7295\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4306 - val_loss: 7.4817\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.3775 - val_loss: 7.6290\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.2679 - val_loss: 7.8180\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.3446 - val_loss: 7.7946\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 1.3120 - val_loss: 7.6546\n",
      "12/12 [==============================] - 0s 6ms/step\n",
      "0.3826681049946946\n",
      "(744, 7073, 20)\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 22.8690 - val_loss: 18.0189\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 12.4426 - val_loss: 14.3098\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 10.8136 - val_loss: 12.5852\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 9.5924 - val_loss: 11.0581\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 8.6318 - val_loss: 10.6065\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 7.5824 - val_loss: 10.3950\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 5.8433 - val_loss: 8.7585\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 4.8975 - val_loss: 7.9098\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 4.2960 - val_loss: 9.3724\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 3.9370 - val_loss: 8.0612\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 3.7738 - val_loss: 7.9464\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 3.0377 - val_loss: 7.7511\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.7571 - val_loss: 7.4312\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 3.0000 - val_loss: 7.5888\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.8801 - val_loss: 7.8001\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.6594 - val_loss: 7.4083\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.6121 - val_loss: 7.1894\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 2.4064 - val_loss: 7.5988\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.5495 - val_loss: 7.5427\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.4689 - val_loss: 7.5073\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.3803 - val_loss: 7.3629\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.3452 - val_loss: 7.4475\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.7584 - val_loss: 6.6804\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.4527 - val_loss: 7.5214\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.4055 - val_loss: 7.5798\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.9002 - val_loss: 8.2953\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 2.0917 - val_loss: 6.7543\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.2635 - val_loss: 6.8516\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.1193 - val_loss: 6.8941\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.7588 - val_loss: 8.0051\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.2064 - val_loss: 7.4792\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.7082 - val_loss: 7.1330\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.8340 - val_loss: 7.1529\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 2.0285 - val_loss: 7.1477\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.8881 - val_loss: 7.4902\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.9710 - val_loss: 8.1569\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.9561 - val_loss: 8.0988\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.0036 - val_loss: 7.2954\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.9558 - val_loss: 7.2416\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.9637 - val_loss: 7.0835\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.8200 - val_loss: 6.8448\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 1.5400 - val_loss: 7.0292\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6803 - val_loss: 7.3554\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.7267 - val_loss: 7.3971\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.9037 - val_loss: 7.2950\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.7854 - val_loss: 7.3496\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5971 - val_loss: 7.1613\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6458 - val_loss: 7.1813\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.0279 - val_loss: 7.0875\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.9562 - val_loss: 7.1139\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.9987 - val_loss: 7.4841\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.6788 - val_loss: 6.7983\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.9148 - val_loss: 6.9058\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.7925 - val_loss: 7.1978\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.7716 - val_loss: 7.9178\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 2.1302 - val_loss: 7.0995\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6729 - val_loss: 7.2484\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6617 - val_loss: 7.1512\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 1.5226 - val_loss: 7.1858\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4820 - val_loss: 6.9171\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.8693 - val_loss: 6.8871\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5812 - val_loss: 6.9260\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.5700 - val_loss: 7.0827\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.5320 - val_loss: 6.8112\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6070 - val_loss: 6.7577\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6180 - val_loss: 6.9020\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5103 - val_loss: 6.9077\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6551 - val_loss: 6.9218\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.5991 - val_loss: 6.8091\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6254 - val_loss: 6.6331\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.5813 - val_loss: 6.9331\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6594 - val_loss: 6.4588\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.3648 - val_loss: 6.6376\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4642 - val_loss: 6.6609\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.4557 - val_loss: 7.0416\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5423 - val_loss: 6.5733\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4700 - val_loss: 6.6447\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.4537 - val_loss: 7.1585\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.9456 - val_loss: 6.9438\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.5240 - val_loss: 6.9483\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.5538 - val_loss: 6.8395\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5901 - val_loss: 6.9686\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4780 - val_loss: 7.0994\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6523 - val_loss: 6.8039\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.6373 - val_loss: 6.8338\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.3676 - val_loss: 6.9198\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.4717 - val_loss: 6.5584\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.2711 - val_loss: 6.6193\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.4897 - val_loss: 6.9622\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.7142 - val_loss: 6.8136\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.3875 - val_loss: 6.6903\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5980 - val_loss: 6.7702\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5326 - val_loss: 6.5939\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.5316 - val_loss: 6.5446\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.4543 - val_loss: 6.3331\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.3370 - val_loss: 6.1765\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.2754 - val_loss: 6.4662\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.1995 - val_loss: 6.5515\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 1.3110 - val_loss: 6.5644\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 1.0650 - val_loss: 6.4861\n",
      "12/12 [==============================] - 0s 7ms/step\n",
      "0.559559637646585\n",
      "Mean R2 across 3 folds: 0.5282\n",
      "Standard Deviation of R2: 0.1084\n",
      "mean of mean square error of R2: 0.1084\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Set the input shape and number of classes (1 for regression)\n",
    "input_shape = (X_train.shape[1:])\n",
    "num_classes = 1\n",
    "\n",
    "# Create the model architecture\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "# Initialize lists to store R2 scores for each fold\n",
    "r2_scores = []\n",
    "mse = []\n",
    "# Create a KFold cross-validator\n",
    "num_folds = 3\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate through cross-validation folds\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    # Split the data into train and validation sets for this fold\n",
    "    X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "    print(X_fold_train.shape)\n",
    "    # Create a new instance of the model for each fold\n",
    "    model = create_model()\n",
    "   \n",
    "    \n",
    "    # Train the model on the current fold's training data\n",
    "    model.fit(X_fold_train, y_fold_train, epochs=100, batch_size=64, validation_data=(X_fold_val, y_fold_val))\n",
    "    \n",
    "    # Evaluate the model on the current fold's validation data\n",
    "    y_fold_val_pred = model.predict(X_fold_val)\n",
    "    mse_i = mean_squared_error(y_fold_val, y_fold_val_pred)\n",
    "    r2 = r2_score(y_fold_val, y_fold_val_pred)\n",
    "    print(r2)\n",
    "    r2_scores.append(r2)\n",
    "    mse.append(mse_i)\n",
    "    \n",
    "\n",
    "# Calculate the mean and standard deviation of R2 scores\n",
    "mean_r2 = np.mean(r2_scores)\n",
    "std_r2 = np.std(r2_scores)\n",
    "mse = np.mean(mse)\n",
    "print(f\"Mean R2 across {num_folds} folds: {mean_r2:.4f}\")\n",
    "print(f\"Standard Deviation of R2: {std_r2:.4f}\")\n",
    "print(f\"mean of mean square error of R2: {std_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e14f52f5-b642-4c1c-96aa-a112754e2bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 16:24:01.575460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-30 16:24:01.589529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-30 16:24:01.589680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-30 16:24:01.590391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-30 16:24:01.590519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-30 16:24:01.590628: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-30 16:24:01.656659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-30 16:24:01.656809: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-30 16:24:01.656924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-30 16:24:01.657001: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:226] Using CUDA malloc Async allocator for GPU: 0\n",
      "2023-08-30 16:24:01.657050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9888 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 16:24:02.771122: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
      "2023-08-30 16:24:03.068262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 3s 95ms/step - loss: 23.6201 - val_loss: 9.0007\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 14.5516 - val_loss: 8.0877\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 11.8413 - val_loss: 9.6039\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 10.8861 - val_loss: 10.3337\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 9.4944 - val_loss: 6.7484\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 7.9475 - val_loss: 6.0481\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 7.0157 - val_loss: 5.7321\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 7.2908 - val_loss: 6.4487\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 6.4802 - val_loss: 5.6956\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 6.1136 - val_loss: 5.4855\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 6.6201 - val_loss: 6.2476\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 5.4809 - val_loss: 5.2725\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 5.6176 - val_loss: 5.4284\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 4.8468 - val_loss: 5.2408\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 4.5093 - val_loss: 5.0896\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 4.6349 - val_loss: 5.2285\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.7220 - val_loss: 6.2003\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 4.2431 - val_loss: 5.5435\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.3523 - val_loss: 5.1323\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.0026 - val_loss: 4.9943\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.1741 - val_loss: 5.1984\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.0604 - val_loss: 5.3328\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.6896 - val_loss: 5.4462\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.0278 - val_loss: 4.9169\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.1437 - val_loss: 5.6273\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 4.0770 - val_loss: 4.8377\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.0339 - val_loss: 4.9832\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.7159 - val_loss: 4.8459\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.8287 - val_loss: 4.9669\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.7244 - val_loss: 5.2351\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.2467 - val_loss: 4.9010\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.1603 - val_loss: 4.9744\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.9992 - val_loss: 4.9496\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.8856 - val_loss: 4.8385\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.8696 - val_loss: 4.8799\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.5579 - val_loss: 4.7166\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.8774 - val_loss: 5.3840\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.1992 - val_loss: 5.1510\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.2889 - val_loss: 5.4095\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.6683 - val_loss: 4.9873\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.7269 - val_loss: 5.0477\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.8064 - val_loss: 5.0660\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.8308 - val_loss: 5.0608\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.9301 - val_loss: 5.4195\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.6339 - val_loss: 4.8772\n",
      "Epoch 46/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.8278 - val_loss: 4.9541\n",
      "Epoch 47/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 3.3566 - val_loss: 4.8911\n",
      "Epoch 48/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 3.0318 - val_loss: 4.8596\n",
      "Epoch 49/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.8442 - val_loss: 4.9593\n",
      "Epoch 50/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.5293 - val_loss: 5.2674\n",
      "Epoch 51/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.9003 - val_loss: 5.0095\n",
      "Epoch 52/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.8500 - val_loss: 5.0301\n",
      "Epoch 53/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.4407 - val_loss: 5.3185\n",
      "Epoch 54/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.5393 - val_loss: 4.8421\n",
      "Epoch 55/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.5070 - val_loss: 5.1314\n",
      "Epoch 56/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.7418 - val_loss: 4.9080\n",
      "Epoch 57/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.7469 - val_loss: 5.0507\n",
      "Epoch 58/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.6862 - val_loss: 4.8482\n",
      "Epoch 59/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.7284 - val_loss: 5.0510\n",
      "Epoch 60/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 3.0255 - val_loss: 4.9214\n",
      "Epoch 61/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.4170 - val_loss: 4.8405\n",
      "Epoch 62/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.3235 - val_loss: 5.0104\n",
      "Epoch 63/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.6155 - val_loss: 4.9166\n",
      "Epoch 64/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.6001 - val_loss: 5.0106\n",
      "Epoch 65/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.2003 - val_loss: 4.9038\n",
      "Epoch 66/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.2895 - val_loss: 5.0456\n",
      "Epoch 67/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.3321 - val_loss: 4.7861\n",
      "Epoch 68/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.4697 - val_loss: 4.9386\n",
      "Epoch 69/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.1422 - val_loss: 5.2560\n",
      "Epoch 70/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.9266 - val_loss: 5.0678\n",
      "Epoch 71/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.5353 - val_loss: 4.9359\n",
      "Epoch 72/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.3491 - val_loss: 4.9264\n",
      "Epoch 73/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.8834 - val_loss: 4.8404\n",
      "Epoch 74/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.4816 - val_loss: 5.0429\n",
      "Epoch 75/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.3407 - val_loss: 5.0143\n",
      "Epoch 76/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.1965 - val_loss: 5.1947\n",
      "Epoch 77/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.4641 - val_loss: 4.9179\n",
      "Epoch 78/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.2762 - val_loss: 5.0396\n",
      "Epoch 79/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.3380 - val_loss: 4.9638\n",
      "Epoch 80/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.7117 - val_loss: 5.0501\n",
      "Epoch 81/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.4973 - val_loss: 5.2067\n",
      "Epoch 82/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.6707 - val_loss: 5.2617\n",
      "Epoch 83/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.1450 - val_loss: 4.9818\n",
      "Epoch 84/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.9321 - val_loss: 4.9321\n",
      "Epoch 85/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.4952 - val_loss: 4.9893\n",
      "Epoch 86/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.2459 - val_loss: 4.9712\n",
      "Epoch 87/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.9429 - val_loss: 5.1796\n",
      "Epoch 88/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.0764 - val_loss: 4.9987\n",
      "Epoch 89/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.0504 - val_loss: 4.8737\n",
      "Epoch 90/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.8760 - val_loss: 4.9910\n",
      "Epoch 91/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.8358 - val_loss: 5.1926\n",
      "Epoch 92/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.9051 - val_loss: 5.0797\n",
      "Epoch 93/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.7192 - val_loss: 4.8128\n",
      "Epoch 94/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.0934 - val_loss: 4.9490\n",
      "Epoch 95/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.1498 - val_loss: 5.1969\n",
      "Epoch 96/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.7609 - val_loss: 5.0501\n",
      "Epoch 97/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 2.1763 - val_loss: 5.5744\n",
      "Epoch 98/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 2.3240 - val_loss: 5.0905\n",
      "Epoch 99/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 2.1117 - val_loss: 5.1353\n",
      "Epoch 100/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 2.1547 - val_loss: 5.5266\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 23.4815 - val_loss: 17.3815\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 14.7545 - val_loss: 15.1655\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 12.3834 - val_loss: 14.8542\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 10.1489 - val_loss: 13.1665\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 9.3403 - val_loss: 13.5766\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 8.6281 - val_loss: 13.4174\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 8.2220 - val_loss: 12.4287\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 7.0450 - val_loss: 12.0505\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 6.5425 - val_loss: 12.4288\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 6.7231 - val_loss: 12.1735\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 5.9910 - val_loss: 12.6910\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 5.4089 - val_loss: 12.1479\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 5.3081 - val_loss: 12.2737\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 4.4492 - val_loss: 12.3286\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 4.0088 - val_loss: 12.5141\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 4.0075 - val_loss: 14.0763\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 4.1076 - val_loss: 12.7988\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 3.4518 - val_loss: 11.7867\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 3.0229 - val_loss: 12.1823\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 3.3931 - val_loss: 11.9458\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 3.2737 - val_loss: 13.1794\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 3.5703 - val_loss: 11.8878\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 3.7533 - val_loss: 11.9841\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 3.4017 - val_loss: 12.0568\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 3.8292 - val_loss: 12.3485\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.9561 - val_loss: 11.9152\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.9652 - val_loss: 12.6342\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 3.3511 - val_loss: 12.1572\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.9337 - val_loss: 12.0622\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 3.2734 - val_loss: 12.4198\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 3.1037 - val_loss: 12.7400\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 3.8175 - val_loss: 12.2542\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.7826 - val_loss: 12.1922\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 3.0381 - val_loss: 12.2164\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.8667 - val_loss: 11.9981\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.5886 - val_loss: 11.7984\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.7285 - val_loss: 12.3221\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.8536 - val_loss: 12.4042\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.6641 - val_loss: 12.3967\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.7366 - val_loss: 12.1340\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.5791 - val_loss: 12.1811\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 3.1330 - val_loss: 11.8993\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.3223 - val_loss: 12.0156\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.7893 - val_loss: 12.1820\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 3.1524 - val_loss: 12.1377\n",
      "Epoch 46/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.8651 - val_loss: 12.4439\n",
      "Epoch 47/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.5578 - val_loss: 12.1943\n",
      "Epoch 48/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.4851 - val_loss: 12.4676\n",
      "Epoch 49/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.5538 - val_loss: 12.2012\n",
      "Epoch 50/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.6514 - val_loss: 12.4630\n",
      "Epoch 51/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.2276 - val_loss: 12.2107\n",
      "Epoch 52/100\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 3.1358 - val_loss: 12.5263\n",
      "Epoch 53/100\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 2.7134 - val_loss: 12.2982\n",
      "Epoch 54/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.4172 - val_loss: 12.1238\n",
      "Epoch 55/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.9511 - val_loss: 12.4025\n",
      "Epoch 56/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.6113 - val_loss: 12.2696\n",
      "Epoch 57/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.6662 - val_loss: 12.4929\n",
      "Epoch 58/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.2639 - val_loss: 12.5138\n",
      "Epoch 59/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.3435 - val_loss: 12.1179\n",
      "Epoch 60/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.6254 - val_loss: 12.5560\n",
      "Epoch 61/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.4126 - val_loss: 12.2027\n",
      "Epoch 62/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.3437 - val_loss: 12.2799\n",
      "Epoch 63/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.2650 - val_loss: 12.1611\n",
      "Epoch 64/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.4816 - val_loss: 12.3125\n",
      "Epoch 65/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.6958 - val_loss: 12.4645\n",
      "Epoch 66/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.3399 - val_loss: 12.5729\n",
      "Epoch 67/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.4237 - val_loss: 12.8367\n",
      "Epoch 68/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.5074 - val_loss: 12.3430\n",
      "Epoch 69/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.9208 - val_loss: 12.7316\n",
      "Epoch 70/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.5510 - val_loss: 12.6816\n",
      "Epoch 71/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.7990 - val_loss: 13.1153\n",
      "Epoch 72/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.4049 - val_loss: 12.5996\n",
      "Epoch 73/100\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.1895 - val_loss: 12.6635\n",
      "Epoch 74/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.6265 - val_loss: 12.5038\n",
      "Epoch 75/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 2.4449 - val_loss: 12.3849\n",
      "Epoch 76/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.3513 - val_loss: 12.3901\n",
      "Epoch 77/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.3183 - val_loss: 12.3033\n",
      "Epoch 78/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 2.2443 - val_loss: 12.3092\n",
      "Epoch 79/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 1.9728 - val_loss: 12.4334\n",
      "Epoch 80/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 2.1672 - val_loss: 12.2298\n",
      "Epoch 81/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 2.4069 - val_loss: 12.4344\n",
      "Epoch 82/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.4061 - val_loss: 12.4548\n",
      "Epoch 83/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.9108 - val_loss: 12.2831\n",
      "Epoch 84/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 2.0278 - val_loss: 12.3120\n",
      "Epoch 85/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.0052 - val_loss: 12.3839\n",
      "Epoch 86/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.0955 - val_loss: 12.3036\n",
      "Epoch 87/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 1.9846 - val_loss: 12.4053\n",
      "Epoch 88/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.3237 - val_loss: 12.9246\n",
      "Epoch 89/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.3863 - val_loss: 12.3076\n",
      "Epoch 90/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.0226 - val_loss: 12.4678\n",
      "Epoch 91/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.1520 - val_loss: 12.3124\n",
      "Epoch 92/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.2687 - val_loss: 12.5197\n",
      "Epoch 93/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.1489 - val_loss: 12.3031\n",
      "Epoch 94/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.8561 - val_loss: 12.4671\n",
      "Epoch 95/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.9267 - val_loss: 12.5346\n",
      "Epoch 96/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 1.5505 - val_loss: 12.2142\n",
      "Epoch 97/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 1.7508 - val_loss: 12.3453\n",
      "Epoch 98/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.2041 - val_loss: 12.4340\n",
      "Epoch 99/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.0632 - val_loss: 12.3270\n",
      "Epoch 100/100\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 1.8728 - val_loss: 12.3665\n",
      "9/9 [==============================] - 0s 7ms/step\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 20.5515 - val_loss: 15.1034\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 12.1934 - val_loss: 14.1061\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 9.8295 - val_loss: 10.0525\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 8.2739 - val_loss: 7.4502\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 6.3846 - val_loss: 6.5814\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 5.7892 - val_loss: 7.2685\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 4.8428 - val_loss: 6.4791\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 4.3738 - val_loss: 6.0337\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 4.3276 - val_loss: 5.8356\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 3.4158 - val_loss: 6.2724\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 3.3899 - val_loss: 6.0452\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 3.2713 - val_loss: 6.0801\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 3.0927 - val_loss: 5.7048\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.8592 - val_loss: 5.9819\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.6898 - val_loss: 5.5493\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.8330 - val_loss: 5.8018\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.8161 - val_loss: 5.8281\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.7246 - val_loss: 5.7642\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.4149 - val_loss: 6.0695\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.2888 - val_loss: 5.7880\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.1814 - val_loss: 5.7203\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.5840 - val_loss: 5.5677\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.2761 - val_loss: 5.4386\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.2312 - val_loss: 5.7147\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.2367 - val_loss: 5.7617\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.2732 - val_loss: 5.4925\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.1041 - val_loss: 5.6705\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.5372 - val_loss: 5.6364\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.2780 - val_loss: 5.4955\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.0372 - val_loss: 6.0401\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.9199 - val_loss: 5.4238\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.0074 - val_loss: 5.6187\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 1.9528 - val_loss: 6.0107\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.9663 - val_loss: 5.6857\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.2471 - val_loss: 5.8830\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.8231 - val_loss: 5.9802\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.9181 - val_loss: 5.8422\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.0653 - val_loss: 5.5326\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.9089 - val_loss: 5.9516\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.9422 - val_loss: 6.5003\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.8961 - val_loss: 5.9149\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.8915 - val_loss: 5.4601\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.7464 - val_loss: 5.5778\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.7162 - val_loss: 5.7235\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.6928 - val_loss: 5.6612\n",
      "Epoch 46/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.8239 - val_loss: 5.4754\n",
      "Epoch 47/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.6879 - val_loss: 5.8302\n",
      "Epoch 48/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.8136 - val_loss: 5.5130\n",
      "Epoch 49/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.8155 - val_loss: 5.3347\n",
      "Epoch 50/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.6713 - val_loss: 6.2844\n",
      "Epoch 51/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.7803 - val_loss: 5.5494\n",
      "Epoch 52/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.1192 - val_loss: 5.6869\n",
      "Epoch 53/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.6737 - val_loss: 5.4242\n",
      "Epoch 54/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.8162 - val_loss: 5.8519\n",
      "Epoch 55/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.6915 - val_loss: 5.2796\n",
      "Epoch 56/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.5209 - val_loss: 5.2101\n",
      "Epoch 57/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.9046 - val_loss: 5.8977\n",
      "Epoch 58/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.6241 - val_loss: 5.5593\n",
      "Epoch 59/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.6889 - val_loss: 5.8234\n",
      "Epoch 60/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.5674 - val_loss: 5.2812\n",
      "Epoch 61/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.8572 - val_loss: 5.6743\n",
      "Epoch 62/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 2.1672 - val_loss: 5.3685\n",
      "Epoch 63/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.7376 - val_loss: 5.7805\n",
      "Epoch 64/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.6449 - val_loss: 5.2545\n",
      "Epoch 65/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.7294 - val_loss: 5.3967\n",
      "Epoch 66/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.6638 - val_loss: 5.3863\n",
      "Epoch 67/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.5867 - val_loss: 5.3629\n",
      "Epoch 68/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.7395 - val_loss: 6.0972\n",
      "Epoch 69/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.8935 - val_loss: 5.1381\n",
      "Epoch 70/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.7524 - val_loss: 5.7794\n",
      "Epoch 71/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.6971 - val_loss: 5.4529\n",
      "Epoch 72/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 1.6514 - val_loss: 5.5375\n",
      "Epoch 73/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.7203 - val_loss: 5.4822\n",
      "Epoch 74/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.6068 - val_loss: 5.3223\n",
      "Epoch 75/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.5586 - val_loss: 5.4185\n",
      "Epoch 76/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.5678 - val_loss: 5.5204\n",
      "Epoch 77/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.4399 - val_loss: 5.9860\n",
      "Epoch 78/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.3148 - val_loss: 5.4110\n",
      "Epoch 79/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.4010 - val_loss: 5.5584\n",
      "Epoch 80/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.7299 - val_loss: 5.3667\n",
      "Epoch 81/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.4233 - val_loss: 5.5059\n",
      "Epoch 82/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.2186 - val_loss: 5.4380\n",
      "Epoch 83/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.2909 - val_loss: 5.7578\n",
      "Epoch 84/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.2626 - val_loss: 5.6269\n",
      "Epoch 85/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.4382 - val_loss: 5.2128\n",
      "Epoch 86/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.4384 - val_loss: 6.1211\n",
      "Epoch 87/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.2599 - val_loss: 5.6154\n",
      "Epoch 88/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.4454 - val_loss: 5.3573\n",
      "Epoch 89/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.4761 - val_loss: 5.4744\n",
      "Epoch 90/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.5715 - val_loss: 5.7975\n",
      "Epoch 91/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.3278 - val_loss: 5.3582\n",
      "Epoch 92/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.2551 - val_loss: 5.8011\n",
      "Epoch 93/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.2654 - val_loss: 5.3956\n",
      "Epoch 94/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.2760 - val_loss: 5.5788\n",
      "Epoch 95/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.4079 - val_loss: 5.6663\n",
      "Epoch 96/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.2082 - val_loss: 5.6510\n",
      "Epoch 97/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.5579 - val_loss: 5.9388\n",
      "Epoch 98/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.6908 - val_loss: 5.5268\n",
      "Epoch 99/100\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 1.8184 - val_loss: 6.2164\n",
      "Epoch 100/100\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 1.4710 - val_loss: 5.9980\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "Mean R2 across 3 folds: 0.4474\n",
      "Standard Deviation of R2: 0.1840\n",
      "Mean of Mean Squared Error: 7.9637\n"
     ]
    }
   ],
   "source": [
    "# num_folds = 3\n",
    "\n",
    "# # Initialize lists to store R2 scores for each fold\n",
    "# r2_scores = []\n",
    "# mse_scores = []\n",
    "\n",
    "# # Create a KFold cross-validator\n",
    "# kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# # Iterate through cross-validation folds\n",
    "# for train_val_idx, test_idx in kf.split(X):\n",
    "#     X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "#     y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "\n",
    "#     # Split train_val into train and validation sets\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # Set the input shape and number of classes (1 for regression)\n",
    "#     input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "#     num_classes = 1\n",
    "    \n",
    "#     # Create a new instance of the model for each fold\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(256, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(num_classes, activation='linear'))\n",
    "    \n",
    "#     model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "    \n",
    "#     # Train the model on the current fold's training data\n",
    "#     model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val))\n",
    "    \n",
    "#     # Evaluate the model on the current fold's validation data\n",
    "#     y_val_pred = model.predict(X_val)\n",
    "#     r2 = r2_score(y_val, y_val_pred)\n",
    "#     mse = mean_squared_error(y_val, y_val_pred)\n",
    "#     r2_scores.append(r2)\n",
    "#     mse_scores.append(mse)\n",
    "\n",
    "# # Calculate the mean and standard deviation of R2 scores\n",
    "# mean_r2 = np.mean(r2_scores)\n",
    "# std_r2 = np.std(r2_scores)\n",
    "# mean_mse = np.mean(mse_scores)\n",
    "# print(f\"Mean R2 across {num_folds} folds: {mean_r2:.4f}\")\n",
    "# print(f\"Standard Deviation of R2: {std_r2:.4f}\")\n",
    "# print(f\"Mean of Mean Squared Error: {mean_mse:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e4519-a20b-4585-a8cf-6f09bc5737a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of folds\n",
    "n_splits = 3  # You can adjust this as needed\n",
    "\n",
    "# Initialize the KFold cross-validator\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "data_splits = {}\n",
    "\n",
    "# Iterate over each fold\n",
    "for count, (train_idx, val_idx) in enumerate(kf.split(X), start=1):\n",
    "    X_fold_train, X_fold_val = X[train_idx], X[val_idx]\n",
    "    y_fold_train, y_fold_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Store the datasets in a dictionary\n",
    "    data_splits[f'fold_{count}'] = {\n",
    "        'X_train': X_fold_train,\n",
    "        'X_val': X_fold_val,\n",
    "        'y_train': y_fold_train,\n",
    "        'y_val': y_fold_val\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fddb03b-6c5c-4b21-81da-dc353f208cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf7614f-78fd-4be3-99c0-10790f7fd33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data for the first fold\n",
    "fold_num = 1\n",
    "fold_data = data_splits[f'fold_{fold_num}']\n",
    "X_fold_train = fold_data['X_train']\n",
    "X_fold_val = fold_data['X_val']\n",
    "y_fold_train = fold_data['y_train']\n",
    "y_fold_val = fold_data['y_val']\n",
    "input_shape = (X_fold_train.shape[1:])\n",
    "num_classes = 1\n",
    "# Now you can use X_fold_train, y_fold_train for training and X_fold_val, y_fold_val for validation\n",
    "# Train and validate your model using this data\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "# Initialize lists to store R2 scores for each fold\n",
    "r2_scores = []\n",
    "\n",
    "# Create a KFold cross-validator\n",
    "num_folds = 3\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate through cross-validation folds\n",
    "\n",
    "    # Split the data into train and validation sets for this fold\n",
    "\n",
    "print(X_fold_train.shape)\n",
    "# Create a new instance of the model for each fold\n",
    "model = create_model()\n",
    "\n",
    "\n",
    "# Train the model on the current fold's training data\n",
    "model.fit(X_fold_train, y_fold_train, epochs=100, batch_size=64)\n",
    "\n",
    "# Evaluate the model on the current fold's validation data\n",
    "y_fold_val_pred = model.predict(X_fold_val)\n",
    "r2 = r2_score(y_fold_val, y_fold_val_pred)\n",
    "mse_i = mean_squared_error(y_fold_val, y_fold_val_pred)\n",
    "print(r2)\n",
    "print(mse_i)\n",
    "r2_scores.append(r2)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93608a-363b-4fd2-b212-3fdcf4b66a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data for the first fold\n",
    "fold_num = 2\n",
    "fold_data = data_splits[f'fold_{fold_num}']\n",
    "X_fold_train = fold_data['X_train']\n",
    "X_fold_val = fold_data['X_val']\n",
    "y_fold_train = fold_data['y_train']\n",
    "y_fold_val = fold_data['y_val']\n",
    "input_shape = (X_fold_train.shape[1:])\n",
    "num_classes = 1\n",
    "# Now you can use X_fold_train, y_fold_train for training and X_fold_val, y_fold_val for validation\n",
    "# Train and validate your model using this data\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "# Initialize lists to store R2 scores for each fold\n",
    "\n",
    "# Create a KFold cross-validator\n",
    "num_folds = 3\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate through cross-validation folds\n",
    "\n",
    "    # Split the data into train and validation sets for this fold\n",
    "\n",
    "print(X_fold_train.shape)\n",
    "# Create a new instance of the model for each fold\n",
    "model = create_model()\n",
    "\n",
    "\n",
    "# Train the model on the current fold's training data\n",
    "model.fit(X_fold_train, y_fold_train, epochs=100, batch_size=64, validation_data=(X_fold_val, y_fold_val))\n",
    "\n",
    "# Evaluate the model on the current fold's validation data\n",
    "y_fold_val_pred = model.predict(X_fold_val)\n",
    "mse_i = mean_squared_error(y_fold_val, y_fold_val_pred)\n",
    "print(r2)\n",
    "print(mse_i)\n",
    "r2_scores.append(r2)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af8313-0ce4-4375-b634-7c1d11afdc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data for the first fold\n",
    "fold_num = 3\n",
    "fold_data = data_splits[f'fold_{fold_num}']\n",
    "X_fold_train = fold_data['X_train']\n",
    "X_fold_val = fold_data['X_val']\n",
    "y_fold_train = fold_data['y_train']\n",
    "y_fold_val = fold_data['y_val']\n",
    "input_shape = (X_fold_train.shape[1:])\n",
    "num_classes = 1\n",
    "# Now you can use X_fold_train, y_fold_train for training and X_fold_val, y_fold_val for validation\n",
    "# Train and validate your model using this data\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "# Initialize lists to store R2 scores for each fold\n",
    "\n",
    "# Create a KFold cross-validator\n",
    "num_folds = 3\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate through cross-validation folds\n",
    "\n",
    "    # Split the data into train and validation sets for this fold\n",
    "\n",
    "print(X_fold_train.shape)\n",
    "# Create a new instance of the model for each fold\n",
    "model = create_model()\n",
    "\n",
    "\n",
    "# Train the model on the current fold's training data\n",
    "model.fit(X_fold_train, y_fold_train, epochs=20, batch_size=5, validation_data=(X_fold_val, y_fold_val))\n",
    "\n",
    "# Evaluate the model on the current fold's validation data\n",
    "y_fold_val_pred = model.predict(X_fold_val)\n",
    "mse_i = mean_squared_error(y_fold_val, y_fold_val_pred)\n",
    "print(r2)\n",
    "print(mse_i)\n",
    "r2_scores.append(r2)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd50efa-b275-43d8-97d7-ca0f0d2874d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "r2_scores = ['0.6112126816674337', '0.490500579327808', '0.4435895074279347']\n",
    "\n",
    "# Convert the R2 scores to floats\n",
    "r2_scores_float = [float(score) for score in r2_scores]\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean_r2 = np.mean(r2_scores_float)\n",
    "std_r2 = np.std(r2_scores_float)\n",
    "\n",
    "print(f\"Mean R2 across {len(r2_scores_float)} folds: {mean_r2:.4f}\")\n",
    "print(f\"Standard Deviation of R2: {std_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a1ecc2-129d-4e37-a0da-65d9dd6ca3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
